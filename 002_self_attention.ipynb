{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf13f54-3b6d-4d69-9f9a-06c3295ad36b",
   "metadata": {},
   "source": [
    "# SELF ATTENTION\n",
    "Costitisce la base dell'architettura Transformer: rappresenta il meccanismo sulla base del quale vengono calcolati gli attention wheigts tra i token presenti nelle diverse posizione della sequenza di input.<br>\n",
    "Obiettivo della self attention è calcolare un context vector, per ciascun elemento presente nella sequenza di input, che combini informazioni da tutti gli altri elementi. In pratica un context vector può essere interpretato con un embedding vector \"arricchito\".\n",
    "\n",
    "Come primo esempio prenderemo in esame una versione semplificata del meccanismo (senza trainable weights.\n",
    "\n",
    "Definiamo innanzi tutto una sequenza di input (con embeddings di dimensione 3 a titolo esemplificativo). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "440d5f18-4e0e-4566-b9f2-d60f02494a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89dd37-f133-48b9-aab5-1dab79b7aba4",
   "metadata": {},
   "source": [
    "Come primo step andiamo a calcolare gli attention scores tra query token e ciascun input token facendo il dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea149658-07f9-4724-82e1-7bbeb00b86de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# The second input token serves as the query. \n",
    "query = inputs[1]                            \n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b0afe-19a3-4a99-a5f5-5ba3548f75ba",
   "metadata": {},
   "source": [
    "Una volta ottenuti gli attention scores è necessario normalizzarli (la somma deve essere  = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd8961f-5c5b-4e66-86ef-f4cdcdf90044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e9799-368f-458c-852d-6008ad24e09b",
   "metadata": {},
   "source": [
    "In realtà per normalization si utilizza Softmax:\n",
    "- gestisce meglio valori estremi \n",
    "- è meglio x favorire il processo di gradient descent)\n",
    "- assicura che tutti i valori siano positivi e quindi che possano essere interpretati come probabilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8840ce74-8f9b-47f9-b29e-4024e65be991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfee63-3350-493f-ab99-41d93e97db90",
   "metadata": {},
   "source": [
    "E' meglio utilizzare l'implementazione di Softmax di Pytorch in quanto più stabile e efficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d450c4-ac9b-4fc3-81f9-a1ceb836a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec6601-9e5b-423c-b05d-f096c3426020",
   "metadata": {},
   "source": [
    "Dopo aver calcolato gli attention weights è possibile calcolare il context vector moltiplicando l'input token con il weight corrispondente e sommando il vettore risultante.<br>\n",
    "Il context vector è quindi la somma ponderata di tutti gli input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6711bb3f-63c7-448e-8216-26d0a024fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# The second input token is the query\n",
    "query = inputs[1]        \n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec82c84-6ba7-471f-b60e-cd932ccd3645",
   "metadata": {},
   "source": [
    "E' ora possibile generalizzare questo processo per calcolare attention weigths per tutti i token.\n",
    "\n",
    "Gli step sono gli stessi di prima tranne che per il fatto che vengono calcolati tutti i context vector anziche uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdafc4a5-6231-40e0-843d-900f1442745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e4e02-1527-418a-b49c-b55b67f4e204",
   "metadata": {},
   "source": [
    "Ciascun elemento del tensor rapprenseta attention score tra ciascuna copia di elementi di input. \n",
    "\n",
    "E' possibile ottenere lo stesso risultato usando matrix multiplication invece del for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b163f35-76df-419a-85ec-667ba1ee1f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ef19b-9359-456f-be2b-df6a7779b942",
   "metadata": {},
   "source": [
    "A questo punto si può procedere con la normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574e5ead-bdae-482e-b2cb-bb7be9b088d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166338bd-ec91-483a-80b1-eee27b240090",
   "metadata": {},
   "source": [
    "Una volta ottenuti gli attention weights (normalizzati) è possibile ottenere il context vector via matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e8c3251-e076-4277-8e42-72eeb091fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d3845a-3976-4714-adbf-ef7adac345b7",
   "metadata": {},
   "source": [
    "# SELF ATTENTION CON TRAINABLE PARAMETERS\n",
    "\n",
    "Rispetto a quanto visto finora vengono introdotti delle weights matrices che verranno aggiornate durante il training del modello.\n",
    "Le weights matrices sono 3 (Wq, Wk, Wv)\n",
    "\n",
    "A titolo di esempio definiamo alcune variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25978fbe-e2a2-4207-a7b3-3d40ca3307f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second input element \n",
    "x_2 = inputs[1]    \n",
    "\n",
    "# The input embedding size, d=3 \n",
    "d_in = inputs.shape[1]      \n",
    "\n",
    "# The output embedding size, d_out=2 \n",
    "d_out = 2         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d09e9-b3ee-4ec5-8636-9b099d3e1971",
   "metadata": {},
   "source": [
    "Inizializiamo 3 weights matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f023e43-054a-44a4-8aad-d78814f3d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af6e4be1-ad2a-4be1-bd0b-26e824182a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb209fa-88b5-45c4-af71-26c02cac276f",
   "metadata": {},
   "source": [
    "Calcolando su tutti itoken di input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87e3078d-a3b7-4937-9779-d3977d0c0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66020dcb-5284-4325-a4e2-c8cb83fa567b",
   "metadata": {},
   "source": [
    "Secondo step è calcolare gli attention score.\n",
    "\n",
    "Inizialmente calcoliamo solo l'attention score per il secondo token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "746a9ec1-f8f9-4424-9313-202a6fb30bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]       \n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f3ef6-8e72-48ee-b045-2a0c2d433f40",
   "metadata": {},
   "source": [
    "Questo può essere generalizzato per calcolare tutti gli attention score via matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d108e44-795b-4022-abc7-bc1825be1432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# All attention scores for given query \n",
    "attn_scores_2 = query_2 @ keys.T     \n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7ab8a-5789-462f-8454-2f9c210bcf79",
   "metadata": {},
   "source": [
    "Per passare da attention scores ad attention weights dobbiamo applicare Softmax dopo aver scalato gli attention scores (dividendo per la radice quadrata della dimensione degli embedding delle key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb974a1b-3448-41a1-8d9d-8671a1596629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd0af4-440a-42e4-b5c0-5c65ad35f4f1",
   "metadata": {},
   "source": [
    "Ultimo step è calcolare i context vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89a9f482-8946-41c2-b1df-9b3e8660b96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce7a52-a9f6-4f5c-a389-7402389ad001",
   "metadata": {},
   "source": [
    "## Self Attention class\n",
    "\n",
    "E' possibile implementare una classe che organizzi i calcoli visti in precedenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40cee262-4640-42e9-bb26-3547775fa7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafbac4-a911-4cf6-bf45-23c054626b4f",
   "metadata": {},
   "source": [
    "Questa classe può essere utilizzata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad80205e-8e18-4972-bd71-4ef2267b2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6dc4b-09e2-43a6-a368-dc717716ce82",
   "metadata": {},
   "source": [
    "E' possibile migliorare la precedente versione della classe utilizzando nn.Linear al posto di nn.Parameter in quanto più efficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f37f53b3-b7d2-47dd-93a3-a2bbcffc4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a48da5e-78ac-4b62-a714-6dfc1003967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce033dd-f188-44d7-b226-50d62e98924c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
